{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week_1-Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Function to plot the binomial distribution for a sequence of n values\n",
    "def plot_binomial_distributions(nlist, p):\n",
    "    for n in nlist:\n",
    "        k = np.arange(0, n+1)\n",
    "        f = binom.pmf(k, n, p)\n",
    "        plt.bar(k, f)\n",
    "        plt.xlabel('Number of Successes')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title(f'Binomial Distribution, p={p} n={n}')\n",
    "        plt.show()\n",
    "\n",
    "# Function to plot the rescaled binomial distributions\n",
    "def plot_rescaled_binomial_distributions(nlist, p, zmax):\n",
    "    for n in nlist:\n",
    "        k = np.arange(0, n+1)\n",
    "        z = (k - n*p) / np.sqrt(n*p*(1-p))\n",
    "        zi = np.abs(z) <= zmax\n",
    "        f = binom.pmf(k, n, p)\n",
    "        plt.bar(z[zi], f[zi])\n",
    "        plt.xlabel('Scaling Variable z')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.title(f'Binomial Distribution, p={p} n={n}')\n",
    "        plt.show()\n",
    "\n",
    "# Parameters\n",
    "nlist = [1, 2, 5, 10, 20, 50, 100, 1000]\n",
    "p = 0.1\n",
    "zmax = 5\n",
    "\n",
    "# Plot the binomial distributions\n",
    "plot_binomial_distributions(nlist, p)\n",
    "\n",
    "# Plot the rescaled binomial distributions\n",
    "plot_rescaled_binomial_distributions(nlist, p, zmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week_2-Introduction_to_Discrete-Time_Stochastic_Processes\n",
    "\n",
    "## AR(1) Process Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "phi = 0.8\n",
    "mu = 0\n",
    "sigma = 1\n",
    "n = 100\n",
    "\n",
    "# Generate white noise\n",
    "epsilon = np.random.normal(0, sigma, n)\n",
    "\n",
    "# Initialize the series\n",
    "X = np.zeros(n)\n",
    "\n",
    "# Simulate the AR(1) process\n",
    "for t in range(1, n):\n",
    "    X[t] = mu + phi * X[t-1] + epsilon[t]\n",
    "\n",
    "# Plot the series\n",
    "plt.plot(X)\n",
    "plt.title('AR(1) Process Simulation')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('X_t')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "title: \"Testing the Random Walk\"\n",
    "author: \"Paul F. Mende\"\n",
    "date: \"Summer 2021\"\n",
    "output: \n",
    "  html_notebook:\n",
    "  df_print: paged\n",
    "  toc: yes\n",
    "```\n",
    "\n",
    "Before we get started, let's install a few **packages**.\n",
    "- The `pip install` command is run **once** to download the software to your computer.\n",
    "- The `import` command is run **one time per session** in order to load a package's functions and make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have never installed them, uncomment the lines below and run it one time.\n",
    "\n",
    "# !pip install yfinance pandas numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll load packages.\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tootsie Roll\n",
    "\n",
    "Let's load some data and look at default summary stats for data from Tootsie Roll (TR).\n",
    "\n",
    "**Technical note:** Data is often exchanged using \"flat files,\" which are plain text files that can be read using a simple text editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch some test data from Yahoo! Finance\n",
    "\n",
    "# Define query parameters\n",
    "ticker = \"TR\"\n",
    "date_first = \"1987-12-31\"\n",
    "date_last = \"2017-12-31\"\n",
    "\n",
    "# Get the data\n",
    "TR = yf.download(ticker, start=date_first, end=date_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what the price looks like over time\n",
    "\n",
    "plt.plot(TR.index, TR['Adj Close'], label='Adjusted Price')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.title('TR Adjusted Price 1988-2017')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the returns\n",
    "P = TR['Adj Close']\n",
    "r = np.diff(np.log(P))\n",
    "N = len(r)\n",
    "\n",
    "# The returns can also be stored as a new column in TR.\n",
    "TR['r'] = np.append([np.nan], r)\n",
    "\n",
    "# Trim off the first row, which has return NA\n",
    "TR = TR.dropna()\n",
    "\n",
    "plt.plot(TR.index, TR['r'], label='Daily Returns')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('TR Daily Returns 1988-2017')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The daily return series is noisy, and the mean value is barely visible.\n",
    "# Compare the graph above with the simulation below, in which simulated returns have the same average volatility and zero mean.\n",
    "\n",
    "plt.plot(TR.index, np.random.normal(scale=np.std(TR['r']), size=len(TR)), label='White Noise')\n",
    "plt.ylim(-0.18, 0.18)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('White Noise Process with TR Volatility')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ## Summary statistics and return distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are high-level summary stats that pandas provides for any data frame.\n",
    "TR.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualization conventions\n",
    "# Annualized return = 252 * (Daily return)\n",
    "# Annualized std. dev. = sqrt(252) * (Daily std. dev)\n",
    "\n",
    "mean_return_annualized = np.mean(r) * 252\n",
    "volatility_annualized = np.std(r) * np.sqrt(252)\n",
    "\n",
    "mean_return_annualized, volatility_annualized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The histogram of returns has fat tails (and therefore a thin middle).\n",
    "\n",
    "plt.hist(r, bins=50)\n",
    "plt.title('Histogram of TR Daily Returns')\n",
    "plt.show()\n",
    "\n",
    "# ## Lo & MacKinlay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Lo & MacKinlay, we ask whether the measured sample variance of returns grows linearly as a function of the observation interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variance = [np.var(np.diff(np.log(P)))]\n",
    "\n",
    "for n in range(2, 101):\n",
    "    Variance.append(np.var(np.diff(np.log(P[::n]))))\n",
    "\n",
    "plt.plot(Variance)\n",
    "plt.xlabel('n')\n",
    "plt.title('Variance of Returns From n-day Observations')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ## Variance and Ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for $\\widehat \\sigma^2_c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_c(X, q):\n",
    "    T = len(X) - 1\n",
    "    mu = (X[-1] - X[0]) / T\n",
    "    m = (T - q) * (T - q + 1) * q / T\n",
    "    sumsq = sum((X[t + 1] - X[t - q + 1] - q * mu) ** 2 for t in range(q, T))\n",
    "    return sumsq / m\n",
    "\n",
    "def z_stat(X, q):\n",
    "    T = len(X) - 1\n",
    "    c = np.sqrt(T * (3 * q) / (2 * (2 * q - 1) * (q - 1)))\n",
    "    M = variance_c(X, q) / variance_c(X, 1) - 1\n",
    "    return c * M\n",
    "\n",
    "Vc = [variance_c(np.log(P), q) for q in range(1, 101)]\n",
    "zstats = [z_stat(np.log(P), q) for q in range(2, 101)]\n",
    "pValues = [2 * (1 - np.abs(np.random.normal(0, 1))) for z in zstats]\n",
    "\n",
    "plt.bar(range(2, 101), zstats)\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('z')\n",
    "plt.title('z Statistics of Variance Ratio Test')\n",
    "plt.show()\n",
    "\n",
    "# ## Interpreting the test statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test statistic $z(q)$ was constructed to be normally distributed as ${\\cal N}(0,1)$ if the data followed a random walk and scaled accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = [np.sqrt(252) * np.std(np.diff(np.log(P)))]\n",
    "for n in range(2, 101):\n",
    "    sigma.append(np.sqrt(252 / n) * np.std(np.diff(np.log(P[::n]))))\n",
    "\n",
    "plt.bar(range(1, 101), sigma)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Standard Deviation (annualized) / sqrt(n)')\n",
    "plt.title('Volatility Scaling of Returns From n-day Observations (TR)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of returns with similar volatility\n",
    "\n",
    "P_MC = np.exp(np.cumsum(np.random.normal(scale=0.02, size=N)))\n",
    "sigma_MC = [np.sqrt(252) * np.std(np.diff(np.log(P_MC)))]\n",
    "\n",
    "for n in range(2, 101):\n",
    "    sigma_MC.append(np.sqrt(252 / n) * np.std(np.diff(np.log(P_MC[::n]))))\n",
    "\n",
    "plt.bar(range(1, 101), sigma_MC)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('Standard Deviation (annualized) / sqrt(n)')\n",
    "plt.title('Volatility Scaling of Returns From n-day Observations (Sim)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "title: \"Time Series Data and Models\"\n",
    "subtitle: \"Model Identification and Estimation\"\n",
    "author: \"Paul F. Mende\"\n",
    "date: \"Summer 2021\"\n",
    "output: \n",
    "  html_notebook:\n",
    "  df_print: paged\n",
    "  toc: yes\n",
    "---\n",
    "\n",
    "# ## The right model?\n",
    "\n",
    "# Making inferences from real-world data and building effective models is a challenging process.  The data rarely fits exactly, and models may stop working.  So it always requires judgment, not just stats and number crunching, in applying modeling and forecasting techniques. For that reason, Monte Carlo simulations provide an excellent testing laboratory for identifcation techniques.  We can be sure that a \"right answer\" exists, then see which analytics identify it and how much uncertainty remains in a best-case scenario.\n",
    "\n",
    "# - Given a model, estimate its parameters\n",
    "# - Given a class of models, determine the best\n",
    "\n",
    "# ## Order determination:  AR(2) Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have never installed them, uncomment the lines below and run it one time.\n",
    "\n",
    "# !pip install numpy pandas matplotlib statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll load packages.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate AR(2) process\n",
    "c_0 = 0.001\n",
    "c_1 = -0.1\n",
    "c_2 = 0.4\n",
    "sigma = 1\n",
    "Nt = 1000\n",
    "r = np.zeros(Nt)\n",
    "z = np.random.normal(size=Nt)\n",
    "\n",
    "for t in range(2, Nt):\n",
    "    r[t] = c_0 + c_1 * r[t - 1] + c_2 * r[t - 2] + sigma * z[t]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(r))\n",
    "plt.title('AR(2) Sample Path')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('r')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(r, title=\"AR(2) Sample Autocorrelation Function\")\n",
    "plot_pacf(r, title=\"AR(2) Sample Partial Autocorrelation Function\")\n",
    "plt.show()\n",
    "\n",
    "# ## Model estimation: AR(2) example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method (1) Numerical estimation using ordinary least squares\n",
    "y = r[2:]\n",
    "x1 = r[1:-1]\n",
    "x2 = r[:-2]\n",
    "\n",
    "X = np.column_stack([x1, x2])\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method (2): Using the arima function\n",
    "model_ar2 = sm.tsa.ARIMA(r, order=(2, 0, 0)).fit()\n",
    "print(model_ar2.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we get the order incorrect?\n",
    "model_ar5 = sm.tsa.ARIMA(r, order=(5, 0, 0)).fit()\n",
    "print(model_ar5.summary())\n",
    "\n",
    "# ## Order determination: MA(2) Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate MA(2) process\n",
    "mu = 0.0\n",
    "sigma = 1.0\n",
    "phi_1 = -0.1\n",
    "phi_2 = 0.4\n",
    "r = np.zeros(Nt)\n",
    "z = np.random.normal(size=Nt)\n",
    "\n",
    "r[0] = mu + sigma * z[0]\n",
    "r[1] = mu + sigma * z[1] + phi_1 * z[0]\n",
    "for t in range(2, Nt):\n",
    "    r[t] = mu + sigma * z[t] + phi_1 * z[t - 1] + phi_2 * z[t - 2]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(r))\n",
    "plt.title('MA(2) Sample Path')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('r')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(r, title=\"MA(2) Sample Autocorrelation Function\")\n",
    "plot_pacf(r, title=\"MA(2) Sample Partial Autocorrelation Function\")\n",
    "plt.show()\n",
    "\n",
    "# ## Model estimation: MA(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ma2 = sm.tsa.ARIMA(r, order=(0, 0, 2)).fit()\n",
    "print(model_ma2.summary())\n",
    "\n",
    "# ## Real data is much harder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch some test data from Yahoo! Finance\n",
    "# If you have never installed them, uncomment the lines below and run it one time.\n",
    "\n",
    "# !pip install yfinance pandas numpy matplotlib\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "# Define query parameters\n",
    "ticker = \"TR\"\n",
    "date_first = \"1987-12-31\"\n",
    "date_last = \"2017-12-31\"\n",
    "\n",
    "# Get the data\n",
    "TR = yf.download(ticker, start=date_first, end=date_last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what the price looks like over time\n",
    "\n",
    "plt.plot(TR.index, TR['Adj Close'], label='Adjusted Price')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.title('TR Adjusted Price 1988-2017')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the returns\n",
    "P = TR['Adj Close']\n",
    "r = np.diff(np.log(P))\n",
    "N = len(r)\n",
    "\n",
    "# The returns can also be stored as a new column in TR.\n",
    "TR['r'] = np.append([np.nan], r)\n",
    "\n",
    "# Trim off the first row, which has return NA\n",
    "TR = TR.dropna()\n",
    "\n",
    "plt.plot(TR.index, TR['r'], label='Daily Returns')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('TR Daily Returns 1988-2017')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the series stationary?\n",
    "plt.plot(TR.index, np.random.normal(0, np.std(TR['r']), len(TR)), label='White Noise')\n",
    "plt.ylim(-0.18, 0.18)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.title('White Noise Process with TR Volatility')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
